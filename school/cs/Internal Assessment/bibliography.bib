
@article{devlin2019,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.},
  archivePrefix = {arXiv},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  file = {/home/oscar/Zotero/storage/L4HICJHI/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf},
  journal = {arXiv:1810.04805 [cs]},
  keywords = {Computer Science - Computation and Language,model},
  language = {en},
  primaryClass = {cs}
}

@misc{graves2018,
  title = {Deep {{Learning}} 7. {{Attention}} and {{Memory}} in {{Deep Learning}} - {{YouTube}}},
  author = {Graves, Alex},
  year = {2018},
  file = {/home/oscar/Zotero/storage/3HNAGSCL/watch.html},
  keywords = {technology}
}

@inproceedings{johanberggren2019,
  title = {Regression or Classification? {{Automated Essay Scoring}} for {{Norwegian}}},
  shorttitle = {Regression or Classification?},
  booktitle = {Proceedings of the {{Fourteenth Workshop}} on {{Innovative Use}} of {{NLP}} for {{Building Educational Applications}}},
  author = {Johan Berggren, Stig and Rama, Taraka and {\O}vrelid, Lilja},
  year = {2019},
  pages = {92--102},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/W19-4409},
  abstract = {In this paper we present first results for the task of Automated Essay Scoring for Norwegian learner language. We analyze a number of properties of this task experimentally and assess (i) the formulation of the task as either regression or classification, (ii) the use of various non-neural and neural machine learning architectures with various types of input representations, and (iii) applying multi-task learning for joint prediction of essay scoring and native language identification. We find that a GRU-based attention model trained in a single-task setting performs best at the AES task.},
  file = {/home/oscar/Zotero/storage/HIE47YW2/Johan Berggren et al. - 2019 - Regression or classification Automated Essay Scor.pdf},
  language = {en}
}

@misc{kim2020,
  title = {Attention in {{Neural Networks}} - 1. {{Introduction}} to Attention Mechanism {$\cdot$} {{Buomsoo Kim}}},
  author = {Kim, Buomsoo},
  year = {2020},
  month = jan,
  howpublished = {https://buomsoo-kim.github.io/attention/2020/01/01/Attention-mechanism-1.md/},
  keywords = {technology}
}

@article{lehanyang2019,
  title = {Automated {{Examination Grading Using Deep Learning Categorization Techniques}}},
  author = {Lehan Yang},
  year = {2019},
  publisher = {{Unpublished}},
  doi = {10.13140/RG.2.2.32497.53607},
  file = {/home/oscar/Zotero/storage/YWXF7CV4/Lehan Yang - 2019 - Automated Examination Grading Using Deep Learning .pdf},
  language = {en}
}

@article{nguyen2016,
  title = {Neural {{Networks}} for {{Automated Essay Grading}}},
  author = {Nguyen, Huyen and Dery, Lucio},
  year = {2016},
  pages = {11},
  abstract = {The biggest obstacle to choosing constructed-response assessments over traditional multiple-choice assessments is the large cost and effort required for scoring. This project is an attempt to use different neural network architectures to build an accurate automated essay grading system to solve this problem.},
  file = {/home/oscar/Zotero/storage/EH6UWLBQ/Nguyen and Dery - Neural Networks for Automated Essay Grading.pdf},
  language = {en}
}

@inproceedings{pulman2005,
  title = {Automatic Short Answer Marking},
  booktitle = {Proceedings of the Second Workshop on {{Building Educational Applications Using NLP}} - {{EdAppsNLP}} 05},
  author = {Pulman, Stephen G. and Sukkarieh, Jana Z.},
  year = {2005},
  pages = {9--16},
  publisher = {{Association for Computational Linguistics}},
  address = {{Ann Arbor, Michigan}},
  doi = {10.3115/1609829.1609831},
  file = {/home/oscar/Zotero/storage/CXQI4HWH/Pulman and Sukkarieh - 2005 - Automatic short answer marking.pdf},
  language = {en}
}

@article{ramalingam2018,
  title = {Automated {{Essay Grading}} Using {{Machine Learning Algorithm}}},
  author = {Ramalingam, V. V. and Pandian, A and Chetry, Prateek and Nigam, Himanshu},
  year = {2018},
  month = apr,
  volume = {1000},
  pages = {012030},
  issn = {1742-6588, 1742-6596},
  doi = {10.1088/1742-6596/1000/1/012030},
  abstract = {Essays are paramount for of assessing the academic excellence along with linking the different ideas with the ability to recall but are notably time consuming when they are assessed manually. Manual grading takes significant amount of evaluator's time and hence it is an expensive process. Automated grading if proven effective will not only reduce the time for assessment but comparing it with human scores will also make the score realistic. The project aims to develop an automated essay assessment system by use of machine learning techniques by classifying a corpus of textual entities into small number of discrete categories, corresponding to possible grades. Linear regression technique will be utilized for training the model along with making the use of various other classifications and clustering techniques. We intend to train classifiers on the training set, make it go through the downloaded dataset, and then measure performance our dataset by comparing the obtained values with the dataset values. We have implemented our model using java.},
  file = {/home/oscar/Zotero/storage/2V2HYB96/Ramalingam et al. - 2018 - Automated Essay Grading using Machine Learning Alg.pdf},
  journal = {Journal of Physics: Conference Series},
  language = {en}
}

@inproceedings{sinha2018,
  title = {Answer {{Evaluation Using Machine Learning}}},
  author = {Sinha, Prince and Kaul, Ayush and Bharadia, Sharad},
  year = {2018},
  pages = {7},
  abstract = {In this modern age, where the world moves towards automation so, there is a need for automation in answer evaluation system. Currently, the online answer evaluation is available for mcq based question, hence evaluation of the theory answer is hectic for the checker. Teacher manually checks the answer and allot the marks. The current system takes more manpower and time to evaluate the answer. In this journal an application based on the evaluation of answers using machine learning. The objective of the journal is to specially reduce the manpower and time consumption. Since in manual answer evaluation, the manpower and the time consumption is much more. Also, in the manual system, it may be possible that the marks given to two same answers are different. This application system provides an automatic evaluation of answer based on the keyword provided to the application in form of the input by the moderator which will provide equal distribution of marks and will reduce time and manpower.},
  file = {/home/oscar/Notes/school/cs/Answer_Evaluation_with_ML.pdf},
  language = {en}
}

@inproceedings{taghipour2016,
  title = {A {{Neural Approach}} to {{Automated Essay Scoring}}},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {Taghipour, Kaveh and Ng, Hwee Tou},
  year = {2016},
  pages = {1882--1891},
  publisher = {{Association for Computational Linguistics}},
  address = {{Austin, Texas}},
  doi = {10.18653/v1/D16-1193},
  abstract = {Traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays. The performance of such systems is tightly bound to the quality of the underlying features. However, it is laborious to manually design the most informative features for such a system. In this paper, we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score, without any feature engineering. We explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models. The results show that our best system, which is based on long short-term memory networks, outperforms a strong baseline by 5.6\% in terms of quadratic weighted Kappa, without requiring any feature engineering.},
  file = {/home/oscar/Zotero/storage/FI5L8T94/Taghipour and Ng - 2016 - A Neural Approach to Automated Essay Scoring.pdf},
  language = {en}
}

@article{vaswani2017,
  title = {Attention Is {{All}} You {{Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  pages = {11},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  file = {/home/oscar/Zotero/storage/L74R8KT7/Vaswani et al. - Attention is All you Need.pdf},
  keywords = {technology},
  language = {en}
}

@article{xu2015,
  title = {Show, {{Attend}} and {{Tell}}: {{Neural Image CaptionGeneration}} with {{Visual Attention}}},
  author = {Xu, Kelvin and Lei, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard S and Bengio, Yoshua},
  year = {2015},
  pages = {10},
  abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-theart performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.},
  file = {/home/oscar/Zotero/storage/VRELXTU8/Xu et al. - Show, Attend and Tell Neural Image CaptionGenerat.pdf},
  keywords = {technology},
  language = {en}
}

@inproceedings{yang2016,
  title = {Hierarchical {{Attention Networks}} for {{Document Classification}}},
  booktitle = {Proceedings of the 2016 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
  year = {2016},
  pages = {1480--1489},
  publisher = {{Association for Computational Linguistics}},
  address = {{San Diego, California}},
  doi = {10.18653/v1/N16-1174},
  abstract = {We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
  file = {/home/oscar/Zotero/storage/G4VTDX3W/Yang et al. - 2016 - Hierarchical Attention Networks for Document Class.pdf},
  keywords = {technology},
  language = {en}
}

@article{yang2020,
  title = {{{XLNet}}: {{Generalized Autoregressive Pretraining}} for {{Language Understanding}}},
  shorttitle = {{{XLNet}}},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
  year = {2020},
  month = jan,
  abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.1.},
  archivePrefix = {arXiv},
  eprint = {1906.08237},
  eprinttype = {arxiv},
  file = {/home/oscar/Zotero/storage/CY5M2ASK/Yang et al. - 2020 - XLNet Generalized Autoregressive Pretraining for .pdf},
  journal = {arXiv:1906.08237 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,model},
  language = {en},
  primaryClass = {cs}
}


