% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nty/global//global/global}
    \entry{devlin2019}{article}{}
      \name{author}{4}{}{%
        {{hash=13202969e372bc82318f9629cbdd199b}{%
           family={Devlin},
           familyi={D\bibinitperiod},
           given={Jacob},
           giveni={J\bibinitperiod}}}%
        {{hash=a45784fe7163b45f11d166564f5d24b6}{%
           family={Chang},
           familyi={C\bibinitperiod},
           given={Ming-Wei},
           giveni={M\bibinithyphendelim W\bibinitperiod}}}%
        {{hash=8dde73b4194f5bc4230c4808f3fc1534}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Kenton},
           giveni={K\bibinitperiod}}}%
        {{hash=b92aa283415413bb8d2a1548716d0c7d}{%
           family={Toutanova},
           familyi={T\bibinitperiod},
           given={Kristina},
           giveni={K\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{ad8e2e2a80d28ade21e86852b804fc9b}
      \strng{fullhash}{e8bccd48302a14eeba57d9dce2f49ef4}
      \strng{bibnamehash}{ad8e2e2a80d28ade21e86852b804fc9b}
      \strng{authorbibnamehash}{ad8e2e2a80d28ade21e86852b804fc9b}
      \strng{authornamehash}{ad8e2e2a80d28ade21e86852b804fc9b}
      \strng{authorfullhash}{e8bccd48302a14eeba57d9dce2f49ef4}
      \field{sortinit}{D}
      \field{sortinithash}{c438b3d5d027251ba63f5ed538d98af5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1810.04805 [cs]}
      \field{month}{5}
      \field{shorttitle}{{{BERT}}}
      \field{title}{{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}}
      \field{year}{2019}
      \verb{eprint}
      \verb 1810.04805
      \endverb
      \verb{file}
      \verb /home/oscar/Zotero/storage/L4HICJHI/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf
      \endverb
      \keyw{Computer Science - Computation and Language,model}
    \endentry
  \enddatalist
\endrefsection
\endinput

